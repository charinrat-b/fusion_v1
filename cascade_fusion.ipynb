{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from python_speech_features import mfcc\n",
    "from scipy.io import wavfile\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = MTCNN()\n",
    "\n",
    "def extract_face_from_img(image):\n",
    "    faces = face_detector.detect_faces(image)\n",
    "    x1, y1, width, height = faces[0]['box']\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_palm_from_img(image):\n",
    "    image = np.rot90(image, 3)  # Rotate 90 degrees clockwise\n",
    "    h, w = image.shape\n",
    "    img = np.zeros((h + 160, w + 160), np.uint8)  # Pad the image by 80 pixels on 4 sides\n",
    "    img[80:-80, 80:-80] = image\n",
    "    # Apply GaussionBlur to remove noise\n",
    "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "    # Apply Binary + OTSU thresholding to generate Black-White image\n",
    "    # White pixels denote the palm and back pixels denote the background\n",
    "    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "    # Section 2\n",
    "    M = cv2.moments(th)\n",
    "    h, w = img.shape\n",
    "    # Get centroid of the white pixels\n",
    "    x_c = M['m10'] // M['m00']\n",
    "    y_c = M['m01'] // M['m00']\n",
    "\n",
    "    # Apply Erosion to remove noise\n",
    "    kernel = np.array([[0, 1, 0],\n",
    "                       [1, 1, 1],\n",
    "                       [0, 1, 0]]).astype(np.uint8)\n",
    "    erosion = cv2.erode(th, kernel, iterations=1)\n",
    "    boundary = th - erosion\n",
    "\n",
    "    cnt, _ = cv2.findContours(boundary, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    img_c = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    areas = [cv2.contourArea(c) for c in cnt]\n",
    "    max_index = np.argmax(areas)\n",
    "    cnt = cnt[max_index]\n",
    "\n",
    "    img_cnt = cv2.drawContours(img_c, [cnt], 0, (255, 0, 0), 2)\n",
    "\n",
    "    cnt = cnt.reshape(-1, 2)\n",
    "    left_id = np.argmin(cnt.sum(-1))\n",
    "    cnt = np.concatenate([cnt[left_id:, :], cnt[:left_id, :]])\n",
    "\n",
    "    # Section 3\n",
    "    dist_c = np.sqrt(np.square(cnt-[x_c, y_c]).sum(-1))\n",
    "    f = np.fft.rfft(dist_c)\n",
    "    cutoff = 15\n",
    "    f_new = np.concatenate([f[:cutoff], 0*f[cutoff:]])\n",
    "    dist_c_1 = np.fft.irfft(f_new)\n",
    "\n",
    "    # Section 4\n",
    "    eta = np.square(np.abs(f_new)).sum()/np.square(np.abs(f)).sum()\n",
    "    # print('Power Retained: {:.4f}{}'.format(eta*100, '%'))\n",
    "\n",
    "    # Section 5\n",
    "    derivative = np.diff(dist_c_1)\n",
    "    sign_change = np.diff(np.sign(derivative))/2\n",
    "\n",
    "    # Section 6\n",
    "    minimas = cnt[np.where(sign_change>0)[0]]\n",
    "    v1, v2 = minimas[-1], minimas[-3]\n",
    "\n",
    "    theta = np.arctan2((v2-v1)[1], (v2-v1)[0])*180/np.pi\n",
    "    R = cv2.getRotationMatrix2D((int(v2[0]), int(v2[1])), theta, 1)\n",
    "\n",
    "    img_r = cv2.warpAffine(img, R, (w, h))\n",
    "    v1 = (R[:, :2] @ v1 + R[:, -1]).astype(np.int)\n",
    "    v2 = (R[:, :2] @ v2 + R[:, -1]).astype(np.int)\n",
    "\n",
    "    ux = v1[0]\n",
    "    uy = v1[1] + (v2-v1)[0]//3\n",
    "    lx = v2[0]\n",
    "    ly = v2[1] + 4*(v2-v1)[0]//3\n",
    "\n",
    "    palm = img_r[uy:ly, ux:lx]\n",
    "    return palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonIDSequence(Sequence):\n",
    "\n",
    "    def __init__(self, csv_file, batch_size, config_file=\"config.json\",\n",
    "                 extract_face=False, extract_palm=False):\n",
    "        self.df = pd.read_csv(csv_file, index_col=0).sample(frac=1)  # Shuffle\n",
    "        self.labels = list(np.unique(self.df.label))\n",
    "        self.num_labels = len(self.labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.extract_face = extract_face\n",
    "        self.extract_palm = extract_palm\n",
    "        with open(config_file) as file:\n",
    "            self.config = json.load(file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.df.shape[0] / self.batch_size)\n",
    "\n",
    "    def load_face(self, img_file):\n",
    "        image = cv2.imread(img_file)[::-1]  # Converting BGR to RGB\n",
    "        if self.extract_face:\n",
    "            image = extract_face_from_img(image)\n",
    "        image = cv2.resize(image, self.config['face_shape'])\n",
    "        image = image.astype(np.float32)\n",
    "        image = preprocess_input(image, version=2)\n",
    "        return image\n",
    "\n",
    "    def load_palm_print(self, image_file):\n",
    "        image = cv2.imread(image_file, 0)  # Read as Gray\n",
    "        if self.extract_palm:\n",
    "            image = extract_palm_from_img(image)\n",
    "        image = cv2.resize(image, self.config['palm_shape'])\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "        image = image * 1./255\n",
    "        return image\n",
    "\n",
    "    def load_signature(self, text_file):\n",
    "        data = np.loadtxt(text_file, skiprows=1, dtype=np.float32)\n",
    "        # Column-wise min-max scaling\n",
    "        diff = data.max(axis=0) - data.min(axis=0)\n",
    "        diff = np.where(diff == 0, 1, diff)  # To handle division-by-zero error\n",
    "        data = (data - data.min(axis=0)) / diff\n",
    "        # Smoothing by rolling-window-mean-subtraction\n",
    "        for i in range(data.shape[1]):\n",
    "            data[:, i] -= pd.Series(data[:, i]).rolling(\n",
    "                window=self.config['rolling_window'], center=True).mean()\n",
    "        if len(data) < self.config['max_strokes']:\n",
    "            pad = self.config['max_strokes'] - len(data)\n",
    "            data = np.pad(data, ((0, pad), (0, 0)))  # Pad at the bottom\n",
    "        else:\n",
    "            n = np.linspace(0, len(data)-1, self.config['max_strokes'],\n",
    "                            dtype=np.int32)\n",
    "            data = data[n]\n",
    "        # data = np.expand_dims(data, axis=-1)\n",
    "        return data\n",
    "\n",
    "    def load_audio(self, audio_file):\n",
    "        def envelop(signal, rate, threshold):\n",
    "            mask = []\n",
    "            y = pd.Series(signal).apply(np.abs)\n",
    "            y_mean = y.rolling(window=int(rate/10), min_periods=1, center=True).mean()\n",
    "            for mean in y_mean:\n",
    "                if mean > threshold:\n",
    "                    mask.append(True)\n",
    "                else:\n",
    "                    mask.append(False)\n",
    "            return mask\n",
    "        rate, signal = wavfile.read(audio_file)\n",
    "        mask = envelop(signal, rate, self.config['audio_clean_threshold'])\n",
    "        signal = signal[mask]\n",
    "        step = int(rate*self.config['audio_seconds'])\n",
    "        rand_idx = np.random.randint(0, signal.shape[0]-step)\n",
    "        sample = signal[rand_idx:rand_idx+step]\n",
    "        sample = mfcc(sample, rate,\n",
    "                      numcep=self.config['audio_numcep'],\n",
    "                      nfilt=self.config['audio_nfilt'],\n",
    "                      nfft=self.config['audio_nfft'])\n",
    "        sample = np.expand_dims(sample, axis=-1)\n",
    "        return sample.astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.df.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        y = batch.pop('label')\n",
    "        X = batch\n",
    "        faces = np.array(list(map(self.load_face, X.face)))\n",
    "        palm_prints = np.array(list(map(self.load_palm_print, X.palm_print)))\n",
    "        audios = np.array(list(map(self.load_audio, X.audio)))\n",
    "        signatures = np.array(list(map(self.load_signature, X.signature)))\n",
    "        y_indices = [to_categorical(self.labels.index(i), num_classes=self.num_labels)\n",
    "                     for i in y.values]\n",
    "        return [faces, palm_prints, audios, signatures], np.array(y_indices)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.df = self.df.sample(frac=1)  # Shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_face_model(input_shape=(224, 224, 3)):\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    vgg16.trainable = False\n",
    "    x = layers.Flatten()(vgg16.output)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "    # x = layers.Dense(1)(x)\n",
    "    model = Model(inputs=vgg16.inputs, outputs=x, name='face_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_palm_print_model(input_shape=(90, 90, 1)):\n",
    "    input_ = layers.Input(shape=(90, 90, 1))\n",
    "    x = layers.Conv2D(32, kernel_size=3, padding='same')(input_)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "    x = layers.Conv2D(32, kernel_size=3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "    x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=x, name='palm_print_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_audio_model_cnn(input_shape=(9, 13, 1)):\n",
    "    input_ = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(16, 3, activation='relu', strides=(1, 1),\n",
    "                      padding='same')(input_)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', strides=(1, 1),\n",
    "                      padding='same')(x)\n",
    "    x = layers.Conv2D( 8, 3, activation='relu', strides=(1, 1),\n",
    "                      padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=x, name='audio_model_cnn')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_audio_model_rnn(input_shape=(9, 13)):\n",
    "    input_ = layers.Input(shape=input_shape)\n",
    "    x = layers.LSTM(128, return_sequences=True)(input_)\n",
    "    x = layers.LSTM(128, return_sequences=True)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(64, activation='relu'))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(32, activation='relu'))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(16, activation='relu'))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(8, activation='relu'))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=x, name='audio_model_rnn')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_signature_model(input_shape=(1000, 5)):\n",
    "    input_ = layers.Input(shape=input_shape)\n",
    "    x = layers.LSTM(128, return_sequences=True)(input_)\n",
    "    x = layers.LSTM(128, return_sequences=True)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(64, activation='relu'))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(32, activation='relu'))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(16, activation='relu'))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(8, activation='relu'))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=x, name='audio_model_rnn')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_model = base_face_model(input_shape=(224, 224, 3))\n",
    "palm_print_model = base_palm_print_model(input_shape=(90,90,1))\n",
    "audio_model = base_audio_model_cnn(input_shape=(9, 13, 1))\n",
    "sign_model = base_signature_model(input_shape=(1000, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_1 = layers.Concatenate(axis=1)([audio_model.output, sign_model.output])\n",
    "merge_1 = layers.Dense(64)(merge_1)\n",
    "merge_1 = layers.BatchNormalization()(merge_1)\n",
    "merge_1 = layers.ReLU()(merge_1)\n",
    "\n",
    "merge_2 = layers.Concatenate(axis=1)([palm_print_model.output, merge_1])\n",
    "merge_2 = layers.Dense(64)(merge_2)\n",
    "merge_2 = layers.BatchNormalization()(merge_2)\n",
    "merge_2 = layers.ReLU()(merge_2)\n",
    "\n",
    "merge_3 = layers.Concatenate(axis=1)([face_model.output, merge_2])\n",
    "merge_3 = layers.BatchNormalization()(merge_3)\n",
    "merge_3 = layers.Dense(300, activation='softmax')(merge_3)\n",
    "\n",
    "model = Model(inputs=[face_model.inputs, palm_print_model.inputs,\n",
    "                      audio_model.inputs, sign_model.inputs], outputs=merge_3)\n",
    "model.compile(optimizer=Adam(0.001), loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  1/777 [..............................] - ETA: 0s - loss: nan - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "train_ds = PersonIDSequence(csv_file='datasets/train.csv', batch_size=32)\n",
    "val_ds = PersonIDSequence(csv_file='datasets/val.csv', batch_size=32)\n",
    "\n",
    "model.fit(train_ds, epochs=2, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
