{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from random import shuffle\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "\n",
    "from python_speech_features import mfcc, logfbank\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 900\n",
      "Eval files: 600\n",
      "Labels: 300\n"
     ]
    }
   ],
   "source": [
    "AUDIOS_DIR = os.path.join(\"../datasets\", \"voxceleb1_wavfile\")\n",
    "num_classes = 300\n",
    "\n",
    "def train_test_split():\n",
    "    labels = sorted(os.listdir(AUDIOS_DIR))[:num_classes]\n",
    "    train_files = []\n",
    "    eval_files = []\n",
    "    for label in labels:\n",
    "        folder = os.path.join(AUDIOS_DIR, label)\n",
    "        files = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith(\".wav\")]\n",
    "        shuffle(files)\n",
    "        split_idx = int(len(files) * 0.7)\n",
    "        train_files.extend(files[:split_idx])\n",
    "        eval_files.extend(files[split_idx:])\n",
    "    return train_files, eval_files, labels\n",
    "\n",
    "train_files, eval_files, labels = train_test_split()\n",
    "print(\"Train files: {}\".format(len(train_files)))\n",
    "print(\"Eval files: {}\".format(len(eval_files)))\n",
    "print(\"Labels: {}\".format(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signals(signals):\n",
    "    nrows = len(signals)\n",
    "    ncols = 1\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, sharex=False, sharey=True, figsize=(10, 20))\n",
    "    fig.suptitle(\"Time Series\", size=16)\n",
    "    for x in range(nrows):\n",
    "        axes[x].set_title(list(signals.keys())[x])\n",
    "        axes[x].plot(list(signals.values())[x])\n",
    "        axes[x].get_xaxis().set_visible(False)\n",
    "        axes[x].get_yaxis().set_visible(True)\n",
    "    fig.set_tight_layout(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-fee753e22fcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0msignals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwavfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mplot_signals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-0c2beff0bdce>\u001b[0m in \u001b[0;36mplot_signals\u001b[1;34m(signals)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mncols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mncols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time Series\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "signals = {}\n",
    "folder = os.listdir(AUDIOS_DIR)[156]\n",
    "folder = os.path.join(AUDIOS_DIR, folder)\n",
    "for file in os.listdir(folder):\n",
    "    filepath = os.path.join(folder, file)\n",
    "    if file.endswith(\".wav\"):\n",
    "        signals[filepath] = wavfile.read(filepath)[1]\n",
    "# plot_signals(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numcep = 13\n",
    "nfilt = 26\n",
    "nfft = 512\n",
    "threshold = 200\n",
    "# n_channels = 1\n",
    "n_channels = 3\n",
    "\n",
    "def features_from_audio(audio_file, secs=0.1, n_samples=5):\n",
    "    def envelop(signal, rate, threshold):\n",
    "        mask = []\n",
    "        y = pd.Series(signal).apply(np.abs)\n",
    "        y_mean = y.rolling(window=int(rate/10), min_periods=1, center=True).mean()\n",
    "        for mean in y_mean:\n",
    "            if mean > threshold:\n",
    "                mask.append(True)\n",
    "            else:\n",
    "                mask.append(False)\n",
    "        return mask\n",
    "    rate, signal = wavfile.read(audio_file)\n",
    "    mask = envelop(signal, rate, threshold)\n",
    "    signal = signal[mask]\n",
    "    step = int(rate*secs)\n",
    "    samples = []\n",
    "    for i in range(n_samples):\n",
    "        rand_idx = np.random.randint(0, signal.shape[0]-step)\n",
    "        sample = signal[rand_idx:rand_idx+step]\n",
    "        sample = mfcc(sample, rate, numcep=numcep, nfilt=nfilt, nfft=nfft).astype(np.float32)\n",
    "        sample = np.expand_dims(sample, axis=-1)\n",
    "        if n_channels == 3:\n",
    "            sample = np.tile(sample, [1, 1, 3])\n",
    "        samples.append(sample)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [01:27<00:00, 10.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [01:08<00:00,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failures: []\n",
      "4500 4500\n",
      "3000 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_audio_features = []\n",
    "train_labels = []\n",
    "\n",
    "eval_audio_features = []\n",
    "eval_labels = []\n",
    "\n",
    "n_samples_per_file = 5\n",
    "# files = train_files + eval_files\n",
    "# files = ['../datasets\\\\voxceleb1_wavfile\\\\157\\\\00004.wav']\n",
    "failures = []\n",
    "for file in tqdm(train_files):\n",
    "    try:\n",
    "        samples = features_from_audio(file, 0.1, n_samples_per_file)\n",
    "    except Exception as e:\n",
    "        failures.append(file)\n",
    "        print(e)\n",
    "    else:\n",
    "        train_audio_features.extend(samples)\n",
    "        train_labels.extend([os.path.basename(os.path.dirname(file)) for _ in range(n_samples_per_file)])\n",
    "        \n",
    "for file in tqdm(eval_files):\n",
    "    try:\n",
    "        samples = features_from_audio(file, 0.1, n_samples_per_file)\n",
    "    except Exception as e:\n",
    "        failures.append(file)\n",
    "        print(e)\n",
    "    else:\n",
    "        eval_audio_features.extend(samples)\n",
    "        eval_labels.extend([os.path.basename(os.path.dirname(file)) for _ in range(n_samples_per_file)])\n",
    "\n",
    "print(\"Failures:\", failures)\n",
    "print(len(train_audio_features), len(train_labels))\n",
    "print(len(eval_audio_features), len(eval_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_audio_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonIDSequence(Sequence):\n",
    "\n",
    "    def __init__(self, features, labels, unique_labels, batch_size):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.unique_labels = unique_labels\n",
    "#         shuffle(self.features)\n",
    "        self.num_labels = len(self.unique_labels)\n",
    "        self.batch_size = batch_size\n",
    "        print(len(self.features), math.ceil(len(self.features) / self.batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.features) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = np.array(self.features[idx * self.batch_size:(idx + 1) * self.batch_size])\n",
    "        y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        y_indices = [to_categorical(self.unique_labels.index(i), num_classes=self.num_labels)\n",
    "                     for i in y]\n",
    "        return features, np.array(y_indices)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "#         shuffle(self.files)\n",
    "#         shuffler = np.random.permutation(len(self.features))\n",
    "#         self.features = self.features[shuffler]\n",
    "#         self.labels = self.labels[shuffler]\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def audio_model():\n",
    "#     input_ = layers.Input(shape=(9, 13, 1))\n",
    "#     x = layers.Conv2D(256, 3, strides=(1, 1), padding='same')(input_)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "    \n",
    "#     x = layers.Conv2D(256, 3, strides=(1, 1), padding='same')(input_)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "    \n",
    "#     x = layers.Conv2D(256, 3, strides=(1, 1), padding='same')(input_)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "    \n",
    "#     x = layers.MaxPooling2D((2, 2))(x)\n",
    "#     x = layers.Flatten()(x)\n",
    "    \n",
    "# #     x = layers.Dropout(0.5)(x)\n",
    "# #     x = layers.Dense(1024, activation='relu')(x)\n",
    "    \n",
    "#     x = layers.Dropout(0.5)(x)\n",
    "#     x = layers.Dense(300, name='last_dense')(x)\n",
    "#     x = layers.Softmax()(x)\n",
    "\n",
    "#     model = Model(inputs=input_, outputs=x, name='audio_model_cnn')\n",
    "#     return model\n",
    "\n",
    "# model = audio_model()\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_model():\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=INPUT_SHAPE)\n",
    "    for layer in vgg16.layers:\n",
    "        layer.trainable = False\n",
    "    x = vgg16.output\n",
    "    x = layers.Flatten()(x)                                # Flatten dimensions to for use in FC layers\n",
    "    x = layers.Dense(4096, activation='relu')(x)\n",
    "    x = layers.Dense(4096, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)                             # Dropout layer to reduce overfitting\n",
    "    x = layers.Dense(len(labels), name=\"last_dense\")(x) \n",
    "    x = layers.Softmax()(x)                                # Softmax for multiclass\n",
    "    return Model(inputs=vgg16.input, outputs=x)\n",
    "\n",
    "model = audio_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 71\n",
      "3000 47\n"
     ]
    }
   ],
   "source": [
    "unique_labels = sorted(np.unique(train_labels))\n",
    "train_ds = PersonIDSequence(train_audio_features, train_labels, unique_labels, batch_size=64)\n",
    "eval_ds = PersonIDSequence(eval_audio_features, eval_labels, unique_labels, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 8s 107ms/step - loss: 4.2470 - accuracy: 0.1669 - val_loss: 6.0239 - val_accuracy: 0.0797\n"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "lr = 0.001\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(lr))\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 4ms/step - loss: 9.5335 - accuracy: 0.0893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.533499859749003, 0.08933333]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"audio_model_e{}_lr{}.h5\".format(epochs, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"audio_model_cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 9, 13, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 13, 256)        2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 13, 256)        1024      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 9, 13, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "last_dense (Dense)           (None, 300)               1843500   \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 300)               0         \n",
      "=================================================================\n",
      "Total params: 1,847,084\n",
      "Trainable params: 1,846,572\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2.0)",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
